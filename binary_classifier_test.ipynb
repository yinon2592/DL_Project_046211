{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOe5xwR2Y16++6C+gSPAsLO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yinon2592/DL_Project_046211/blob/main/section_a_classifier_check.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "# drive.mount('/content/drive/my-drive/project_calculations')\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0G11E3mp8rwU",
        "outputId": "26decbee-9e69-47f3-9086-18d08cc72d33"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "! pip install transformers\n",
        "from transformers import set_seed, GPT2Config, GPT2Tokenizer, GPT2ForSequenceClassification\n",
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "import re\n",
        "import torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XfVlP1g8t62",
        "outputId": "e5b362f7-9744-4744-9e4a-2fdabe9cf362"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0liQP-X8gU8",
        "outputId": "aacd3a61-81df-43f1-e2e6-dfe4a1667f84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading last model parameters..\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, OpenAIGPTForSequenceClassification\n",
        "\n",
        "model_config = GPT2Config.from_pretrained('gpt2', num_labels=2) # Binary Classification\n",
        "model = GPT2ForSequenceClassification.from_pretrained('gpt2', config=model_config)\n",
        "section_a_all_loss, section_a_all_acc, section_a_model_description = None, None, None\n",
        "section_a_model_description\n",
        "\n",
        "load_model_parameters = True\n",
        "model_path = '/content/drive/My Drive/project_calculations/section_a_option_1_classifier/'\n",
        "if os.path.exists(model_path + 'section_a_model.pth') and load_model_parameters:\n",
        "  print(\"loading model parameters..\")\n",
        "  model.load_state_dict(torch.load(model_path + 'section_a_model.pth'))\n",
        "  section_a_all_loss = pd.read_csv(model_path + 'section_a_all_loss.csv')\n",
        "  section_a_all_acc = pd.read_csv(model_path + 'section_a_all_acc.csv')\n",
        "  section_a_model_description = pd.read_csv(model_path + 'section_a_model_description.txt')\n",
        "\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.padding_side = \"left\" # Very Important\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.config.pad_token_id = model.config.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_path = '/content/drive/My Drive/project_dataset/test_data.csv'\n",
        "df = pd.read_csv(test_data_path)\n",
        "df = df.sample(1000)\n",
        "print(\"dataset size is \", df.shape[0])\n",
        "print(df.label.value_counts())\n",
        "print(df.sample(5), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_5_KfAO9CZM",
        "outputId": "a8adf0e3-3ca7-4229-8b0b-53a31a302416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset size is  1000\n",
            "0    506\n",
            "1    494\n",
            "Name: label, dtype: int64\n",
            "        label                                               text\n",
            "11685       0  a pipe or something broke my beach trip is a n...\n",
            "527078      1  well you can brave the cold water then warning...\n",
            "178896      0  i have just returned from hell of a day spent ...\n",
            "123090      0                                     don t leave me\n",
            "472022      1                            im 1 day old on twitter \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text input for sanity check of sentiment classification\n",
        "text = \"I really enjoyed the movie. It was fantastic!\"\n",
        "\n",
        "# Tokenize the text\n",
        "encoded_input = tokenizer(text, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "# Perform sentiment classification\n",
        "output = model(**encoded_input)\n",
        "\n",
        "# Retrieve the predicted label and associated probabilities\n",
        "predicted_label = output.logits.argmax().item()\n",
        "predicted_probabilities = output.logits.softmax(dim=1).tolist()[0]\n",
        "\n",
        "# Map the predicted label to sentiment class\n",
        "sentiment_classes = [\"Negative\", \"Positive\"]\n",
        "predicted_sentiment = sentiment_classes[predicted_label]\n",
        "\n",
        "# Print the predicted sentiment and associated probabilities\n",
        "print(\"Predicted Sentiment:\", predicted_sentiment)\n",
        "print(\"Sentiment Probabilities:\", predicted_probabilities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjCkOv8i9nnx",
        "outputId": "f222ac70-f952-42ec-96ed-09ed1832911d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Sentiment: Positive\n",
            "Sentiment Probabilities: [0.2827112674713135, 0.7172887325286865]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TweetDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        super().__init__()\n",
        "        self.data = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        record = self.data.iloc[index]\n",
        "        text = record['text']\n",
        "        return {'text': text, 'label': record['label']}"
      ],
      "metadata": {
        "id": "ggUZUyl6-p1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "test_dataset = TweetDataset(dataset=df)\n",
        "\n",
        "class Gpt2ClassificationCollator(object):\n",
        "    def __init__(self, tokenizer, max_seq_len=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        return\n",
        "\n",
        "    def __call__(self, sequences):\n",
        "        texts = [str(sequence['text']) + \"\\nwas the previous text positive or negative\" for sequence in sequences]\n",
        "        # print(texts)\n",
        "        labels = [int(sequence['label']) for sequence in sequences]\n",
        "        # print(labels)\n",
        "        inputs = self.tokenizer(text=texts,\n",
        "                                return_tensors='pt',\n",
        "                                padding=True,\n",
        "                                truncation=True,\n",
        "                                max_length=self.max_seq_len)\n",
        "        inputs.update({'labels': torch.tensor(labels)})\n",
        "\n",
        "        return inputs\n",
        "\n",
        "gpt2classificationcollator = Gpt2ClassificationCollator(tokenizer=tokenizer,\n",
        "                                                        max_seq_len=512)\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_dataset,\n",
        "                            batch_size=32,\n",
        "                            shuffle=False,\n",
        "                            collate_fn=gpt2classificationcollator)"
      ],
      "metadata": {
        "id": "Ak3Pw0dK-ZBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, device_):\n",
        "    global model\n",
        "    model.eval()\n",
        "\n",
        "    prediction_labels = []\n",
        "    true_labels = []\n",
        "\n",
        "    total_loss = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
        "        batch = {k:v.type(torch.long).to(device_) for k, v in batch.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "            loss, logits = outputs[:2]\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            total_loss.append(loss.item())\n",
        "\n",
        "            prediction_labels += logits.argmax(axis=-1).flatten().tolist()\n",
        "\n",
        "    return true_labels, prediction_labels, total_loss"
      ],
      "metadata": {
        "id": "B9hgCAHE-5sU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import time\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "\n",
        "y, y_pred, val_loss = test(test_dataloader, device)\n",
        "test_acc = accuracy_score(y, y_pred)\n",
        "\n",
        "print(test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvgBx88M_VYA",
        "outputId": "437b64fc-2a38-4b36-8692-d831ef84db2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.66\n"
          ]
        }
      ]
    }
  ]
}
