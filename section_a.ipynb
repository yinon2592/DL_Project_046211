{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNeAIZTQz0VOo0GHUUmCliw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yinon2592/DL_Project_046211/blob/main/section_a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3ZWYdGiSMy_l"
      },
      "outputs": [],
      "source": [
        "# using https://www.kaggle.com/code/baekseungyun/gpt-2-with-huggingface-pytorch\n",
        "\n",
        "# initail lr = 1e-5\n",
        "# we start from 20_000 tweets, after 10 epochs we got into plato\n",
        "# then we try 100_000 tweets for 1 epoch and got 0.04 val improvement\n",
        "# then we try 500_000 tweets for 1 epoch and got 0.08 val improvement\n",
        "# now we try lr = 1e-3\n",
        "\n",
        "\n",
        "# we use data for training in section_a (classifier) and section_c (fine tunning with sentiments), therfore we will split the data for each data separately\n",
        "# we use data for testing to compare \"reverce sentiment capability\" of prompt engineering(section_b) with raw gpt2 vs the finetuned one(section_c), therfore we will use the same validation set for this part"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# drive.mount('/content/drive/my-drive/project_calculations')\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ITQR-LfCmTMW",
        "outputId": "00a6bd9b-72ee-490b-9789-dc5200c9f476",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers\n",
        "from transformers import set_seed, GPT2Config, GPT2Tokenizer, GPT2ForSequenceClassification\n",
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "import re\n",
        "import torch\n",
        "\n",
        "set_seed(731) # My Birthday!, you should get train_loss: 0.773, train_acc: 0.567 in epoch 0.\n",
        "\n",
        "model_config = GPT2Config.from_pretrained('gpt2', num_labels=2) # Binary Classification\n",
        "model = GPT2ForSequenceClassification.from_pretrained('gpt2', config=model_config)\n",
        "\n",
        "load_model_parameters = False\n",
        "model_path = '/content/drive/My Drive/project_calculations/model.pth'\n",
        "if os.path.exists(model_path) and load_model_parameters:\n",
        "  print(\"loading last model parameters..\")\n",
        "  model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.padding_side = \"left\" # Very Important\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.config.pad_token_id = model.config.eos_token_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orGl-3IjUr24",
        "outputId": "a56cae0a-de31-4ae5-ee64-040f5da789f0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Dataset Preparation\n",
        "# load section_a data (data already cleaned)\n",
        "section_a_data_path = '/content/drive/My Drive/project_dataset/section_a_data.csv'\n",
        "df = pd.read_csv(section_a_data_path)\n",
        "df = df.head(500_000)\n",
        "print(\"dataset size is \", df.shape[0])\n",
        "print(df.label.value_counts())\n",
        "print(df.sample(5), \"\\n\")\n",
        "\n",
        "train_size = int(len(df) * 0.8)  # 80% for training\n",
        "print(\"train_size is \", train_size)\n",
        "val_size = len(df) - train_size  # remaining for validation\n",
        "print(\"val_size is \", val_size)\n",
        "\n",
        "train_dataset = df[:train_size]  # First train_size rows for training\n",
        "val_dataset = df[train_size:train_size+val_size]  # Remaining val_size rows for validation\n",
        "\n",
        "class TweetDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        super().__init__()\n",
        "        self.data = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        record = self.data.iloc[index]\n",
        "        text = record['text']\n",
        "        return {'text': text, 'label': record['label']}\n",
        "\n",
        "train_dataset = TweetDataset(dataset=train_dataset)\n",
        "val_dataset = TweetDataset(dataset=val_dataset)\n",
        "\n",
        "# train_dataset = TweetDataset(train=True)\n",
        "# test_dataset = TweetDataset(train=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8F409IVaM7j6",
        "outputId": "8c3a24f8-b6ff-4cd2-8b97-03c77312f185"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset size is  500000\n",
            "1    250022\n",
            "0    249978\n",
            "Name: label, dtype: int64\n",
            "        label                                               text\n",
            "10760       0        so little time until i move out of my house\n",
            "227547      1  i m sorry tell yourself that the rain is what ...\n",
            "291335      1  jesus the heavy rotation tour is almost starti...\n",
            "348028      0  is no longer hungover but really doesn t want ...\n",
            "48892       1                                  morgen housequake \n",
            "\n",
            "train_size is  400000\n",
            "val_size is  100000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Gpt2ClassificationCollator(object):\n",
        "    def __init__(self, tokenizer, max_seq_len=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        return\n",
        "\n",
        "    def __call__(self, sequences):\n",
        "        texts = [str(sequence['text']) for sequence in sequences]\n",
        "        # print(texts)\n",
        "        labels = [int(sequence['label']) for sequence in sequences]\n",
        "        # print(labels)\n",
        "        inputs = self.tokenizer(text=texts,\n",
        "                                return_tensors='pt',\n",
        "                                padding=True,\n",
        "                                truncation=True,\n",
        "                                max_length=self.max_seq_len)\n",
        "        inputs.update({'labels': torch.tensor(labels)})\n",
        "\n",
        "        return inputs\n",
        "\n",
        "gpt2classificationcollator = Gpt2ClassificationCollator(tokenizer=tokenizer,\n",
        "                                                        max_seq_len=60)"
      ],
      "metadata": {
        "id": "Bpn7DaSEOBu5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# train_size = int(len(df) * 0.8)\n",
        "\n",
        "# val_size = len(df) - train_size\n",
        "\n",
        "# # Splitting the data deterministically\n",
        "# train_dataset = df[:train_size]\n",
        "# val_dataset = df[train_size:train_size+val_size]\n",
        "# # train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_dataset,\n",
        "                              batch_size=32,\n",
        "                              shuffle=True,\n",
        "                              collate_fn=gpt2classificationcollator)\n",
        "val_dataloader = DataLoader(dataset=val_dataset,\n",
        "                            batch_size=32,\n",
        "                            shuffle=False,\n",
        "                            collate_fn=gpt2classificationcollator)\n",
        "# test_dataloader = DataLoader(dataset=test_dataset,\n",
        "#                              batch_size=32,\n",
        "#                              shuffle=False,\n",
        "#                              collate_fn=gpt2classificationcollator)\n",
        "\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# true_labels = []\n",
        "# train_dataset\n",
        "# for batch in train_dataloader:\n",
        "#     true_labels += batch['labels'].numpy().flatten().tolist()\n",
        "#     batch = {k:v.type(torch.long).to(device) for k, v in batch.items()}"
      ],
      "metadata": {
        "id": "1qDDVZc2QPp5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW, get_cosine_schedule_with_warmup\n",
        "\n",
        "# last_layers_names =[\n",
        "#   'transformer.h.11.mlp.c_fc.weight',\n",
        "#   'transformer.h.11.mlp.c_fc.bias',\n",
        "#   'transformer.h.11.mlp.c_proj.weight',\n",
        "#   'transformer.h.11.mlp.c_proj.bias',\n",
        "#   'transformer.ln_f.weight',\n",
        "#   'transformer.ln_f.bias',\n",
        "#   'score.weight'\n",
        "# ]\n",
        "\n",
        "# # Set require_grad to False for all levels except the last one\n",
        "# parameters = []\n",
        "# for name, param in model.named_parameters():\n",
        "#   # print(\"name = \", name, \" \", \"type = \", type(name), \" \", \"param = \", param)\n",
        "#   if not name in last_layers_names:  # Exclude parameters of the last layer\n",
        "#     param.requires_grad = False\n",
        "#   else:\n",
        "#     print(\"name = \", name)\n",
        "#     param.requires_grad = True\n",
        "#     parameters.append(param)\n",
        "\n",
        "\n",
        "\n",
        "# param_optimizer = list(model.named_parameters() model.score.parameters())\n",
        "# no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "# optimizer_grouped_parameters = [\n",
        "#     {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "#     {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "# ]\n",
        "# optimizer = AdamW(optimizer_grouped_parameters,\n",
        "#                   lr=1e-5,\n",
        "#                   eps=1e-8)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(\"model.classifier.parameters() = \", model.classifier.parameters())\n",
        "\n",
        "# param_optimizer = {'params': list(model.score.parameters())}\n",
        "\n",
        "# print(param_optimizer)\n",
        "# no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "# optimizer_grouped_parameters = [\n",
        "#     {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "#     {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "# ]\n",
        "# print(list(model.parameters()))\n",
        "\n",
        "# Get the parameters of the model that require gradients\n",
        "# parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
        "# parameters = [param for name, param in model.named_parameters() if param.requires_grad]\n",
        "# print(\"parameters = \", parameters)\n",
        "\n",
        "# params = [param for name, param in model.named_parameters() if param.requires_grad]\n",
        "# print(model.named_parameters())\n",
        "# optimizer = AdamW(parameters,\n",
        "#                   lr=1e-3,\n",
        "#                   eps=1e-8)\n",
        "\n",
        "total_epochs = 20\n",
        "\n",
        "print(model)\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters,\n",
        "                  lr=1e-5,\n",
        "                  eps=1e-8)\n",
        "\n",
        "num_train_steps = len(train_dataloader) * total_epochs\n",
        "num_warmup_steps = int(num_train_steps * 0.1)\n",
        "\n",
        "lr_scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
        "                                               num_warmup_steps=num_warmup_steps,\n",
        "                                               num_training_steps = num_train_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNzmoM86Qa1w",
        "outputId": "67f7658b-505b-4000-f8b9-d36112602ea6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2ForSequenceClassification(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def train(dataloader, optimizer, scheduler, device_):\n",
        "    global model\n",
        "    model.train()\n",
        "\n",
        "    prediction_labels = []\n",
        "    true_labels = []\n",
        "\n",
        "    total_loss = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
        "        batch = {k:v.type(torch.long).to(device_) for k, v in batch.items()}\n",
        "\n",
        "\n",
        "        outputs = model(**batch)\n",
        "        loss, logits = outputs[:2]\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        total_loss.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # prevent exploding gradient\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        prediction_labels += logits.argmax(axis=-1).flatten().tolist()\n",
        "\n",
        "    return true_labels, prediction_labels, total_loss\n",
        "\n",
        "def validation(dataloader, device_):\n",
        "    global model\n",
        "    model.eval()\n",
        "\n",
        "    prediction_labels = []\n",
        "    true_labels = []\n",
        "\n",
        "    total_loss = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
        "        batch = {k:v.type(torch.long).to(device_) for k, v in batch.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "            loss, logits = outputs[:2]\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            total_loss.append(loss.item())\n",
        "\n",
        "            prediction_labels += logits.argmax(axis=-1).flatten().tolist()\n",
        "\n",
        "    return true_labels, prediction_labels, total_loss"
      ],
      "metadata": {
        "id": "DC7MPWLFQfNu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import time\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "\n",
        "all_loss = {'train_loss': [], 'val_loss': []}\n",
        "all_acc = {'train_acc': [], 'val_acc': []}\n",
        "\n",
        "best_val = 0\n",
        "\n",
        "for epoch in range(total_epochs):\n",
        "\n",
        "    start_time = time.time()  # Start the timer\n",
        "    y, y_pred, train_loss = train(train_dataloader, optimizer, lr_scheduler, device)\n",
        "    train_acc = accuracy_score(y, y_pred)\n",
        "\n",
        "    y, y_pred, val_loss = validation(val_dataloader, device)\n",
        "    val_acc = accuracy_score(y, y_pred)\n",
        "\n",
        "    # all_loss['train_loss'] += train_loss\n",
        "    # all_loss['val_loss'] += val_loss\n",
        "\n",
        "    all_loss['train_loss'].append(train_loss)\n",
        "    all_loss['val_loss'].append(val_loss)\n",
        "\n",
        "\n",
        "    all_acc['train_acc'].append(train_acc)\n",
        "    all_acc['val_acc'].append(val_acc)\n",
        "\n",
        "    end_time = time.time()  # Stop the timer\n",
        "    epoch_duration = end_time - start_time\n",
        "\n",
        "    if val_acc > best_val:\n",
        "      best_val = val_acc\n",
        "      # Save your model to Google Drive\n",
        "      print(\" current best val_acc is \", best_val)\n",
        "      torch.save(model.state_dict(), '/content/drive/My Drive/project_calculations/section_a_model.pth')\n",
        "\n",
        "    print(f\"Epoch: {epoch}, Train Loss: {torch.tensor(train_loss).mean():.3f}, Train Acc: {train_acc:.3f}, Val Loss: {torch.tensor(val_loss).mean():.3f}, Val Acc: {val_acc:.3f}, Epoch Duration: {epoch_duration:.3f} seconds\")\n",
        "    pd.DataFrame.from_dict(all_loss).to_csv('/content/drive/My Drive/project_calculations/section_a_all_loss.csv', index=False)\n",
        "    pd.DataFrame.from_dict(all_acc).to_csv('/content/drive/My Drive/project_calculations/section_a_all_acc.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5XbxS0DQoD_",
        "outputId": "50a73eae-aa06-4694-f2c2-71f6dc5a864a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " current best val_acc is  0.51294\n",
            "Epoch: 0, Train Loss: 1.227, Train Acc: 0.511, Val Loss: 0.715, Val Acc: 0.513, Epoch Duration: 910.766 seconds\n",
            " current best val_acc is  0.70496\n",
            "Epoch: 1, Train Loss: 0.685, Train Acc: 0.609, Val Loss: 0.599, Val Acc: 0.705, Epoch Duration: 900.729 seconds\n",
            " current best val_acc is  0.70801\n",
            "Epoch: 2, Train Loss: 0.644, Train Acc: 0.644, Val Loss: 0.568, Val Acc: 0.708, Epoch Duration: 904.381 seconds\n",
            "Epoch: 3, Train Loss: 0.675, Train Acc: 0.647, Val Loss: 0.830, Val Acc: 0.548, Epoch Duration: 904.537 seconds\n",
            " current best val_acc is  0.71462\n",
            "Epoch: 4, Train Loss: 0.749, Train Acc: 0.628, Val Loss: 0.565, Val Acc: 0.715, Epoch Duration: 906.320 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "fig = plt.figure(figsize=(20,20))\n",
        "a = fig.add_subplot(4, 1, 1)\n",
        "b = fig.add_subplot(4, 1, 2)\n",
        "c = fig.add_subplot(2, 1, 2)\n",
        "a.plot(all_loss['train_loss'])\n",
        "b.plot(all_loss['val_loss'])\n",
        "c.plot(all_acc['train_acc'])\n",
        "c.plot(all_acc['val_acc'])\n",
        "c.set(xlabel='epoch', ylabel='accuracy')\n",
        "c.legend(['train', 'val'])"
      ],
      "metadata": {
        "id": "P7sb4rK1fgHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# true_labels, prediction_labels, total_loss = validation(test_dataloader, device)"
      ],
      "metadata": {
        "id": "-DTc45kmjLo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Calculate accuracy\n",
        "# accuracy = sum(1 for true, pred in zip(true_labels, prediction_labels) if true == pred) / len(true_labels)\n",
        "\n",
        "# print(f\"Accuracy percentages: {accuracy * 100}\")\n",
        "\n",
        "# # print(type(true_labels))\n",
        "\n",
        "\n",
        "# # # _, y_pred, _ = validation(test_dataloader, device)\n",
        "# # import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# # print(\"loss per batch: \", total_loss)\n",
        "\n",
        "# # # Convert softmax probabilities to predicted labels\n",
        "# # predicted_labels = np.argmax(prediction_labels, axis=1)\n",
        "\n",
        "# # # Calculate accuracy\n",
        "# # accuracy = np.mean(true_labels == predicted_labels)\n",
        "\n",
        "# # print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# # # submit = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\n",
        "# # # submit['target'] = y_pred\n",
        "\n",
        "# # # submit.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "S5_hzyIlfhyU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}