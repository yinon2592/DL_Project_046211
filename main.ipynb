{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPjUFAa/ov9jvmG6mdNza/L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yinon2592/DL_Project_046211/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **picking sentiment classifier**"
      ],
      "metadata": {
        "id": "aeSeSZy1bg5a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hj8qxFimIpZT",
        "outputId": "a858de1e-e867-4164-ca3e-aa486d600143"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Sentiment: Positive\n",
            "Sentiment Probabilities: [0.00012409620103426278, 0.9998759031295776]\n"
          ]
        }
      ],
      "source": [
        "# !pip install transformers\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# Load the pre-trained sentiment classification model and tokenizer\n",
        "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
        "sentiment_classifier_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "sentiment_classifier_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Text input for sentiment classification\n",
        "text = \"I really enjoyed the movie. It was fantastic!\"\n",
        "\n",
        "# Tokenize the text\n",
        "encoded_input = sentiment_classifier_tokenizer(text, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "# Perform sentiment classification\n",
        "output = sentiment_classifier_model(**encoded_input)\n",
        "\n",
        "# Retrieve the predicted label and associated probabilities\n",
        "predicted_label = output.logits.argmax().item()\n",
        "predicted_probabilities = output.logits.softmax(dim=1).tolist()[0]\n",
        "\n",
        "# Map the predicted label to sentiment class\n",
        "sentiment_classes = [\"Negative\", \"Positive\"]\n",
        "predicted_sentiment = sentiment_classes[predicted_label]\n",
        "\n",
        "# Print the predicted sentiment and associated probabilities\n",
        "print(\"Predicted Sentiment:\", predicted_sentiment)\n",
        "print(\"Sentiment Probabilities:\", predicted_probabilities)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Download 'sentiment140' Data and clean it**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "21d6jGdPb0jK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p data\n",
        "!wget -nc https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/investigating-sentiment-analysis/data/training.1600000.processed.noemoticon.csv.zip -P data\n",
        "!unzip -n -d data data/training.1600000.processed.noemoticon.csv.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0PytSMmRwLR",
        "outputId": "db1d86e6-b70c-43a7-fb77-58166ee36475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘data/training.1600000.processed.noemoticon.csv.zip’ already there; not retrieving.\n",
            "\n",
            "Archive:  data/training.1600000.processed.noemoticon.csv.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "# Step 1: Dataset Preparation\n",
        "file_path=\"data/training.1600000.processed.noemoticon.csv\"\n",
        "df = pd.read_csv(file_path, encoding='ISO-8859-1', header=None)\n",
        "df = df[[0, 5]]\n",
        "df.columns = ['label', 'text']\n",
        "df = df.sample(100, random_state=1)\n",
        "df['label'] = df['label'].replace({0: 'negative', 2: 'neutral', 4: 'positive'})\n",
        "# Drop the rows with 'neutral' sentiment\n",
        "df = df[df['label'] != 'neutral']\n",
        "print(df.label.value_counts())\n",
        "print(df.sample(5))\n",
        "\n",
        "# Step 2: Data Preprocessing\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'@[A-Za-z0-9]+', '', text)  # Remove usernames\n",
        "    text = re.sub(r'https?://[A-Za-z0-9./]+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text)  # Remove special characters\n",
        "    # Remove newlines and multiple whitespaces\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Remove special characters and punctuations\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    text = text.lower().strip()\n",
        "    return text\n",
        "\n",
        "df['text'] = df['text'].apply(clean_text)\n",
        "df['generated_sentence'] = \"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NEISeJ3UK3b",
        "outputId": "170e7805-f68f-4168-e4b9-a4df0e47f44e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative    57\n",
            "positive    43\n",
            "Name: label, dtype: int64\n",
            "            label                                               text\n",
            "26208    negative                                @drummrboy I can't \n",
            "1505267  positive                             DONE! Now I can relax \n",
            "16105    negative  Ugh... Feeling like death warmed up. All of us...\n",
            "1244923  positive  @serenebabe Well as long as it's used for good...\n",
            "657012   negative               @supercoolkp In Oxford that month.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **prompt engineer gpt2 to rephrase sentenses with opposite sentiment**"
      ],
      "metadata": {
        "id": "6KtbdhHGcaLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Model Loading\n",
        "model_name = 'gpt2'\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Step 4: Sentence Generation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, row in df.iterrows():\n",
        "        sentiment = row['label']\n",
        "        opposite_sentiment = 'positive' if sentiment == 'negative' else 'negative'  # Determine the opposite sentiment\n",
        "        input_prompt = f\"rephrase the sentence to {opposite_sentiment} sentiment: {row['text']}\"\n",
        "        input_ids = tokenizer.encode(input_prompt, add_special_tokens=True, return_tensors='pt').to(device)\n",
        "\n",
        "        # Check if input_ids is not None and has a valid shape\n",
        "        if input_ids is not None and input_ids.shape[-1] > 0:\n",
        "            generated_ids = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=input_ids.ne(0),  # Pass attention mask to the model (assuming pad_token_id is 0)\n",
        "                max_length=100,\n",
        "                num_return_sequences=1,\n",
        "                pad_token_id=0  # Set pad_token_id to 0\n",
        "            )\n",
        "            generated_sentence = tokenizer.decode(generated_ids[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "\n",
        "            df.at[i, 'generated_sentence'] = generated_sentence\n",
        "        else:\n",
        "            df.at[i, 'generated_sentence'] = \"\"  # Assign an empty string if input_ids is None or has shape (0, )\n",
        "\n",
        "# Print the generated sentences\n",
        "# print(df['generated_sentence'])\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4GxCxO1Q5Jj",
        "outputId": "29d25953-886f-4075-b3ea-b2ec8a97bb3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           label                                               text  \\\n",
            "514293  negative  i miss nikki nu nu already shes always there w...   \n",
            "142282  negative  so i had a dream last night i remember a sign ...   \n",
            "403727  negative  ohh poor sickly you hugs hope you feel a littl...   \n",
            "649503  negative                                it is raining again   \n",
            "610789  negative                         wish i was in la right now   \n",
            "...          ...                                                ...   \n",
            "505068  negative             gumbo love and alirenco has boy issues   \n",
            "471797  negative             must get into scc basketball next term   \n",
            "460065  negative  had a great afternoon at the lake with the gom...   \n",
            "129162  negative                               i feel neglected sad   \n",
            "202644  negative  how can we enjoy our 2 yr together maybe we wi...   \n",
            "\n",
            "                                       generated_sentence  \n",
            "514293  \\n\\nI'm not sure if this is a good idea or not...  \n",
            "142282  . i was so excited i thought i would be able t...  \n",
            "403727  .\\n\\nThe next day, I was in the hospital with ...  \n",
            "649503  .\\n\\nThe word \"rain\" is used in the same way a...  \n",
            "610789  .\\n\\nThe next day, I was in the middle of a co...  \n",
            "...                                                   ...  \n",
            "505068  .\\n\\nThe word \"alirenco\" is a Latin word meani...  \n",
            "471797  .\\n\\nThe next time you hear a word like \"scc,\"...  \n",
            "460065  .\\n\\nI'm not sure if this is a good idea or no...  \n",
            "129162  , sad, sad, sad, sad, sad, sad, sad, sad, sad,...  \n",
            "202644   to be good and i will be good and i will be g...  \n",
            "\n",
            "[100 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **check accuracy with sentiment classifier**"
      ],
      "metadata": {
        "id": "DppX4OOLcvWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list to store the accuracy values\n",
        "accuracies = []\n",
        "\n",
        "# sentiment_classifier_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "# sentiment_classifier_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Iterate through the DataFrame\n",
        "for i, row in df.iterrows():\n",
        "    original_sentence = row['text']\n",
        "    opposite_sentence = row['generated_sentence']\n",
        "\n",
        "    # Encode the original and opposite sentences\n",
        "    original_input = sentiment_classifier_tokenizer(original_sentence, truncation=True, padding=True, return_tensors='pt')\n",
        "    opposite_input = sentiment_classifier_tokenizer(opposite_sentence, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "    # Get the input IDs and attention mask tensors\n",
        "    original_input_ids = original_input['input_ids'].to(device)\n",
        "    original_attention_mask = original_input['attention_mask'].to(device)\n",
        "    opposite_input_ids = opposite_input['input_ids'].to(device)\n",
        "    opposite_attention_mask = opposite_input['attention_mask'].to(device)\n",
        "\n",
        "    # Move the sentiment classification model to the desired device\n",
        "    sentiment_classifier_model = sentiment_classifier_model.to(device)\n",
        "\n",
        "    # Make predictions for the original and opposite sentences\n",
        "    with torch.no_grad():\n",
        "        original_outputs = sentiment_classifier_model(original_input_ids, attention_mask=original_attention_mask)\n",
        "        opposite_outputs = sentiment_classifier_model(opposite_input_ids, attention_mask=opposite_attention_mask)\n",
        "\n",
        "    # Get the predicted labels\n",
        "    original_predicted_label = torch.argmax(original_outputs.logits).item()\n",
        "    opposite_predicted_label = torch.argmax(opposite_outputs.logits).item()\n",
        "\n",
        "    # Check if the opposite rephrase has the opposite sentiment label\n",
        "    is_opposite = original_predicted_label != opposite_predicted_label\n",
        "\n",
        "    # Calculate accuracy and add it to the list\n",
        "    accuracy = 1 if is_opposite else 0\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Calculate the overall accuracy\n",
        "overall_accuracy = sum(accuracies) / len(accuracies)\n",
        "\n",
        "# Print the overall accuracy\n",
        "print(f\"Accuracy: {overall_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79A2nYwuYrxi",
        "outputId": "8e4bd4ba-1180-4507-ff01-9f9501b094ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.39\n"
          ]
        }
      ]
    }
  ]
}